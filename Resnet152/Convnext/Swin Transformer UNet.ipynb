{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKAQ2bTq+PqphJO9+Lft6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YUJI-AKAMATSU/jm/blob/main/Resnet152/Convnext/Swin%20Transformer%20UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNet with ResNet152  **"
      ],
      "metadata": {
        "id": "AK8H2SuO-Bhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 重要   num_classes: int = 40\n",
        "    epochs: int = 200\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    image_size: tuple = (512, 256)**"
      ],
      "metadata": {
        "id": "XMFkSVhX-Q4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "#     Omnicampus向け：全コード + Scheduler & EarlyStopping + Fix CE Error\n",
        "# ================================\n",
        "\n",
        "# 必要ライブラリ\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet152\n",
        "from torch.amp import autocast, GradScaler\n",
        "import zipfile\n",
        "import shutil\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ------------------\n",
        "#    データの展開\n",
        "# ------------------\n",
        "zip_path = \"/content/data.zip\"\n",
        "extract_dir = \"/content/unzipped\"\n",
        "target_data_dir = \"/content/data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "found = False\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    if 'train' in dirs and 'test' in dirs:\n",
        "        train_path = os.path.join(root, 'train')\n",
        "        test_path = os.path.join(root, 'test')\n",
        "        os.makedirs(target_data_dir, exist_ok=True)\n",
        "        try:\n",
        "            shutil.move(train_path, os.path.join(target_data_dir, 'train'))\n",
        "            shutil.move(test_path, os.path.join(target_data_dir, 'test'))\n",
        "        except shutil.Error:\n",
        "            print(f\"⚠️ 既に {target_data_dir} に train/test が存在しています。移動をスキップします。\")\n",
        "        else:\n",
        "            print(f\"✔️ train/test を {target_data_dir} に移動しました\")\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "if not found:\n",
        "    print(\"⚠️ train/test フォルダが見つかりませんでした。zip の中身を再確認してください。\")\n",
        "\n",
        "shutil.rmtree(extract_dir)\n",
        "\n",
        "# ------------------\n",
        "#  Albumentations Transform\n",
        "# ------------------\n",
        "class AlbumentationsTransform:\n",
        "    def __init__(self, height, width, is_train=True):\n",
        "        if is_train:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(height, width),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.2),\n",
        "                A.Rotate(limit=10, p=0.5),\n",
        "                A.Normalize(),\n",
        "                ToTensorV2()\n",
        "            ], additional_targets={'depth': 'image', 'label': 'mask'})\n",
        "        else:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(height, width),\n",
        "                A.Normalize(),\n",
        "                ToTensorV2()\n",
        "            ], additional_targets={'depth': 'image', 'label': 'mask'})\n",
        "\n",
        "    def __call__(self, image, depth, label=None):\n",
        "        data = {\"image\": np.array(image), \"depth\": np.array(depth)}\n",
        "        if label is not None:\n",
        "            data[\"label\"] = np.array(label)\n",
        "            aug = self.transform(**data)\n",
        "            aug[\"label\"] = aug[\"label\"].long()  # 修正点：すでにTensorなので変換のみ\n",
        "            return aug[\"image\"], aug[\"depth\"], aug[\"label\"]\n",
        "        else:\n",
        "            aug = self.transform(**data)\n",
        "            return aug[\"image\"], aug[\"depth\"]\n",
        "\n",
        "# ------------------\n",
        "#    Dataset Class\n",
        "# ------------------\n",
        "class NYUv2(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.img_dir = os.path.join(root, split, 'image')\n",
        "        self.depth_dir = os.path.join(root, split, 'depth')\n",
        "        self.label_dir = os.path.join(root, split, 'label')\n",
        "        self.img_names = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert(\"RGB\")\n",
        "        depth = Image.open(os.path.join(self.depth_dir, self.img_names[idx]))\n",
        "\n",
        "        if self.split == 'train':\n",
        "            label = Image.open(os.path.join(self.label_dir, self.img_names[idx]))\n",
        "            image, depth, label = self.transform(image, depth, label)\n",
        "            return image, depth, label\n",
        "        else:\n",
        "            image, depth = self.transform(image, depth)\n",
        "            return image, depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "\n",
        "# ------------------\n",
        "#    UNet with ResNet152\n",
        "# ------------------\n",
        "from torchvision.models import resnet152, ResNet152_Weights\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class ResNet152EncoderUNet(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels=4):\n",
        "        super().__init__()\n",
        "        base_model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
        "\n",
        "        # 入力層（RGB+Depthの4chに対応）\n",
        "        self.input_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        with torch.no_grad():\n",
        "            pretrained_w = base_model.conv1.weight\n",
        "            self.input_conv.weight[:, :3] = pretrained_w\n",
        "            self.input_conv.weight[:, 3:] = pretrained_w.mean(dim=1, keepdim=True)\n",
        "\n",
        "        self.input_bn = base_model.bn1\n",
        "        self.input_relu = base_model.relu\n",
        "        self.maxpool = base_model.maxpool\n",
        "\n",
        "        # エンコーダ部（ResNet152のblockを使用）\n",
        "        self.layer1 = base_model.layer1  # 256\n",
        "        self.layer2 = base_model.layer2  # 512\n",
        "        self.layer3 = base_model.layer3  # 1024\n",
        "        self.layer4 = base_model.layer4  # 2048\n",
        "\n",
        "        # デコーダ部（Upsampling + Concat + Conv）\n",
        "        self.up4 = self._up_block(2048, 1024)\n",
        "        self.up3 = self._up_block(1024 + 1024, 512)\n",
        "        self.up2 = self._up_block(512 + 512, 256)\n",
        "        self.up1 = self._up_block(256 + 256, 128)\n",
        "\n",
        "        # 出力層\n",
        "        self.final = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "\n",
        "    def _up_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # エンコーダパス\n",
        "        x = self.input_relu(self.input_bn(self.input_conv(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        e1 = self.layer1(x)\n",
        "        e2 = self.layer2(e1)\n",
        "        e3 = self.layer3(e2)\n",
        "        e4 = self.layer4(e3)\n",
        "\n",
        "        # デコーダパス + skip connection（center crop）\n",
        "        d4 = TF.center_crop(self.up4(e4), e3.shape[-2:])\n",
        "        d3 = TF.center_crop(self.up3(torch.cat([d4, e3], dim=1)), e2.shape[-2:])\n",
        "        d2 = TF.center_crop(self.up2(torch.cat([d3, e2], dim=1)), e1.shape[-2:])\n",
        "        d1 = self.up1(torch.cat([d2, e1], dim=1))\n",
        "\n",
        "        return self.final(d1)\n",
        "\n",
        "\n",
        "# ------------------\n",
        "#    Config + Loader\n",
        "# ------------------\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    dataset_root: str = \"/content/data\"\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 0\n",
        "    in_channels: int = 4\n",
        "    num_classes: int = 40\n",
        "    epochs: int = 200\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    image_size: tuple = (512, 256)\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "config = TrainingConfig()\n",
        "\n",
        "train_transform = AlbumentationsTransform(config.image_size[1], config.image_size[0], is_train=True)\n",
        "test_transform = AlbumentationsTransform(config.image_size[1], config.image_size[0], is_train=False)\n",
        "\n",
        "train_dataset = NYUv2(config.dataset_root, 'train', transform=train_transform)\n",
        "test_dataset = NYUv2(config.dataset_root, 'test', transform=test_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# ------------------\n",
        "#    Model, Loss, Train\n",
        "# ------------------\n",
        "model = ResNet152EncoderUNet(config.num_classes, config.in_channels).to(config.device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        num_classes = preds.shape[1]\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "        mask = targets != 255\n",
        "        targets = targets.clone()\n",
        "        targets[~mask] = 0\n",
        "        one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
        "        mask = mask.unsqueeze(1).float()\n",
        "        preds, one_hot = preds * mask, one_hot * mask\n",
        "        intersection = (preds * one_hot).sum(dim=(2, 3))\n",
        "        union = preds.sum(dim=(2, 3)) + one_hot.sum(dim=(2, 3))\n",
        "        return 1 - ((2 * intersection + self.smooth) / (union + self.smooth)).mean()\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    ce = F.cross_entropy(pred, target, ignore_index=255)\n",
        "    dice = DiceLoss()(pred, target)\n",
        "    return 0.5 * ce + 0.5 * dice\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    total_loss = 0\n",
        "    for img, depth, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        img, depth, label = img.to(config.device), depth.to(config.device), label.to(config.device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type=config.device, enabled=torch.cuda.is_available()):\n",
        "            x = torch.cat((img, depth), dim=1)\n",
        "            pred = model(x)\n",
        "            if pred.shape[-2:] != label.shape[-2:]:\n",
        "                pred = F.interpolate(pred, size=label.shape[-2:], mode='bilinear', align_corners=True)\n",
        "            loss = combined_loss(pred, label)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{config.epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ------------------\n",
        "#    Save + Predict\n",
        "# ------------------\n",
        "save_path = f\"model_{time.strftime('%Y%m%d_%H%M%S')}.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved:\", save_path)\n",
        "\n",
        "model.eval()\n",
        "def predict_tta(model, img, depth):\n",
        "    with torch.no_grad():\n",
        "        x = torch.cat((img, depth), dim=1)\n",
        "        pred1 = model(x)\n",
        "        pred2 = model(torch.flip(x, dims=[3]))\n",
        "        pred2 = torch.flip(pred2, dims=[3])\n",
        "        pred = (pred1 + pred2) / 2\n",
        "        pred = F.interpolate(pred, size=img.shape[-2:], mode='bilinear', align_corners=True)\n",
        "        return pred.argmax(dim=1)\n",
        "\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for img, depth in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
        "        img, depth = img.to(config.device), depth.to(config.device)\n",
        "        pred = predict_tta(model, img, depth)\n",
        "        preds.append(pred.cpu())\n",
        "\n",
        "np.save(\"submission.npy\", torch.cat(preds).numpy())\n",
        "print(\"submission.npy saved\")"
      ],
      "metadata": {
        "id": "YxpXYK4F92G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "#    Evaluation with TTA\n",
        "# ------------------\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ 保存ファイル名を一貫させる\n",
        "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
        "model_path = f\"model_{timestamp}.pt\"\n",
        "submission_npy_path = \"submission.npy\"\n",
        "submission_zip_path = os.path.join(\"/content\", \"submission.zip\")\n",
        "notebook_path = \"/content/DL_Basic_2025_Competition_NYUv2_baseline.ipynb\"  # 必要に応じて変更\n",
        "\n",
        "# ✅ モデル保存\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"✅ Saved: {model_path}\")\n",
        "\n",
        "# ✅ モデル読み込み\n",
        "device = config.device\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ✅ TTA予測関数\n",
        "def predict_tta(model, image, depth, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.cat((image, depth), dim=1)\n",
        "        pred1 = model(x)\n",
        "        x_flip = torch.flip(x, dims=[3])\n",
        "        pred2 = model(x_flip)\n",
        "        pred2 = torch.flip(pred2, dims=[3])\n",
        "        pred = (pred1 + pred2) / 2.0\n",
        "        pred = F.interpolate(pred, size=image.shape[-2:], mode='bilinear', align_corners=True)\n",
        "        pred = pred.argmax(dim=1)\n",
        "    return pred\n",
        "\n",
        "# ✅ TTAで予測\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    print(\"Generating predictions with TTA...\")\n",
        "    for image, depth in tqdm(test_loader):\n",
        "        image, depth = image.to(device), depth.to(device)\n",
        "        pred = predict_tta(model, image, depth, device)\n",
        "        predictions.append(pred.cpu())\n",
        "\n",
        "# ✅ numpy に変換して保存\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "np.save(submission_npy_path, predictions.numpy())\n",
        "print(f\"✅ Predictions saved to: {submission_npy_path}\")\n",
        "\n",
        "# ✅ ZIP 提出ファイル作成\n",
        "with ZipFile(submission_zip_path, mode=\"w\", compression=ZIP_DEFLATED, compresslevel=9) as zf:\n",
        "    zf.write(submission_npy_path, arcname=\"submission.npy\")\n",
        "    zf.write(model_path, arcname=os.path.basename(model_path))\n",
        "    zf.write(notebook_path, arcname=os.path.basename(notebook_path))\n",
        "\n",
        "print(f\"✅ submission.zip saved to: {submission_zip_path}\")\n"
      ],
      "metadata": {
        "id": "vcYT8TiH97VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "#    ZIP 提出ファイル作成（ローカル保存用）\n",
        "# ------------------\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "output_dir = \"/content\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "submission_zip_path = os.path.join(output_dir, \"submission.zip\")\n",
        "notebook_path = \"/content/DL_Basic_2025_Competition_NYUv2_baseline.ipynb\"  # 必要に応じて変更\n",
        "submission_npy_path = \"submission.npy\"  # 作業ディレクトリにある想定\n",
        "\n",
        "with ZipFile(submission_zip_path, mode=\"w\", compression=ZIP_DEFLATED, compresslevel=9) as zf:\n",
        "    zf.write(submission_npy_path, arcname=\"submission.npy\")\n",
        "    zf.write(model_path, arcname=os.path.basename(model_path))\n",
        "    zf.write(notebook_path, arcname=os.path.basename(notebook_path))\n",
        "\n",
        "print(f\"✅ submission.zip saved to: {submission_zip_path}\")"
      ],
      "metadata": {
        "id": "BG31z0tZ99E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/submission.zip\")"
      ],
      "metadata": {
        "id": "T_opoDDd99LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCKO1Y9f-XyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ConvNeXtEncoderUNet Swin Transformer UNet**"
      ],
      "metadata": {
        "id": "LK-XP4ta-bNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E18mWMHo9Xie"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "#     Omnicampus向け：全コード + Scheduler & Fix CE Error（EarlyStopping 除去）\n",
        "# ================================\n",
        "\n",
        "# 必要ライブラリ\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet152\n",
        "from torch.amp import autocast, GradScaler\n",
        "import zipfile\n",
        "import shutil\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "# ------------------\n",
        "#    データの展開\n",
        "# ------------------\n",
        "zip_path = \"/content/data.zip\"\n",
        "extract_dir = \"/content/unzipped\"\n",
        "target_data_dir = \"/content/data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "found = False\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    if 'train' in dirs and 'test' in dirs:\n",
        "        train_path = os.path.join(root, 'train')\n",
        "        test_path = os.path.join(root, 'test')\n",
        "        os.makedirs(target_data_dir, exist_ok=True)\n",
        "        try:\n",
        "            shutil.move(train_path, os.path.join(target_data_dir, 'train'))\n",
        "            shutil.move(test_path, os.path.join(target_data_dir, 'test'))\n",
        "        except shutil.Error:\n",
        "            print(f\"⚠️ 既に {target_data_dir} に train/test が存在しています。移動をスキップします。\")\n",
        "        else:\n",
        "            print(f\"✔️ train/test を {target_data_dir} に移動しました\")\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "if not found:\n",
        "    print(\"⚠️ train/test フォルダが見つかりませんでした。zip の中身を再確認してください。\")\n",
        "\n",
        "shutil.rmtree(extract_dir)\n",
        "\n",
        "# ------------------\n",
        "#  Albumentations Transform\n",
        "# ------------------\n",
        "class AlbumentationsTransform:\n",
        "    def __init__(self, height, width, is_train=True):\n",
        "        if is_train:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(height, width),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.2),\n",
        "                A.Rotate(limit=10, p=0.5),\n",
        "                A.Normalize(),\n",
        "                ToTensorV2()\n",
        "            ], additional_targets={'depth': 'image', 'label': 'mask'})\n",
        "        else:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(height, width),\n",
        "                A.Normalize(),\n",
        "                ToTensorV2()\n",
        "            ], additional_targets={'depth': 'image', 'label': 'mask'})\n",
        "\n",
        "    def __call__(self, image, depth, label=None):\n",
        "        data = {\"image\": np.array(image), \"depth\": np.array(depth)}\n",
        "        if label is not None:\n",
        "            data[\"label\"] = np.array(label)\n",
        "            aug = self.transform(**data)\n",
        "            aug[\"label\"] = aug[\"label\"].long()\n",
        "            return aug[\"image\"], aug[\"depth\"], aug[\"label\"]\n",
        "        else:\n",
        "            aug = self.transform(**data)\n",
        "            return aug[\"image\"], aug[\"depth\"]\n",
        "\n",
        "# ------------------\n",
        "#    Dataset Class\n",
        "# ------------------\n",
        "class NYUv2(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.img_dir = os.path.join(root, split, 'image')\n",
        "        self.depth_dir = os.path.join(root, split, 'depth')\n",
        "        self.label_dir = os.path.join(root, split, 'label')\n",
        "        self.img_names = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert(\"RGB\")\n",
        "        depth = Image.open(os.path.join(self.depth_dir, self.img_names[idx]))\n",
        "\n",
        "        if self.split == 'train':\n",
        "            label = Image.open(os.path.join(self.label_dir, self.img_names[idx]))\n",
        "            image, depth, label = self.transform(image, depth, label)\n",
        "            return image, depth, label\n",
        "        else:\n",
        "            image, depth = self.transform(image, depth)\n",
        "            return image, depth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "# --- Input Adapter（共通）---\n",
        "class InputAdapter(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# --- ConvNeXtBaseEncoderUNet ---\n",
        "class ConvNeXtBaseEncoderUNet(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        self.input_adapter = InputAdapter(in_channels, 3)\n",
        "        self.encoder = timm.create_model(\n",
        "            'convnext_base',\n",
        "            features_only=True,\n",
        "            pretrained=True\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(self.encoder.feature_info[-1]['num_chs'], 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_adapter(x)         # 4ch → 3ch変換\n",
        "        features = self.encoder(x)       # features[-1] = 最終特徴マップ\n",
        "        return self.decoder(features[-1])\n",
        "\n",
        "\n",
        "# --- ConvNeXt UNet ---\n",
        "class ConvNeXtEncoderUNet(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        self.input_adapter = InputAdapter(in_channels, 3)\n",
        "        self.encoder = timm.create_model('convnext_tiny', features_only=True, pretrained=True)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(self.encoder.feature_info[-1]['num_chs'], 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_adapter(x)\n",
        "        features = self.encoder(x)\n",
        "        return self.decoder(features[-1])\n",
        "\n",
        "# --- Swin Transformer UNet ---\n",
        "class SwinEncoderUNet(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        self.input_adapter = InputAdapter(in_channels, 3)\n",
        "        self.encoder = timm.create_model('swin_tiny_patch4_window7_224', features_only=True, pretrained=True)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(self.encoder.feature_info[-1]['num_chs'], 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_adapter(x)\n",
        "        features = self.encoder(x)   # list of features\n",
        "        x = features[-1]             # x.shape: [B, C, H, W] ← 通常はこれでOK\n",
        "        if x.ndim == 4 and x.shape[1] != self.encoder.feature_info[-1]['num_chs']:\n",
        "            # 転置が必要な場合（例：出力が [B, H, W, C] の場合）\n",
        "            x = x.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
        "        return self.decoder(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "#    Config + Loader\n",
        "# ------------------\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    dataset_root: str = \"/content/data\"\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 0\n",
        "    in_channels: int = 4\n",
        "    num_classes: int = 13\n",
        "    epochs: int = 100\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    image_size: tuple = (512, 256) #(224, 224)\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "config = TrainingConfig()\n",
        "\n",
        "train_transform = AlbumentationsTransform(config.image_size[1], config.image_size[0], is_train=True)\n",
        "test_transform = AlbumentationsTransform(config.image_size[1], config.image_size[0], is_train=False)\n",
        "\n",
        "train_dataset = NYUv2(config.dataset_root, 'train', transform=train_transform)\n",
        "test_dataset = NYUv2(config.dataset_root, 'test', transform=test_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# ------------------\n",
        "#    Model, Loss, Train\n",
        "# ------------------\n",
        "\n",
        "# 使用するエンコーダUNetを切り替える（必要に応じてコメントアウト/切り替え）\n",
        "# model = ResNet152EncoderUNet(config.num_classes, config.in_channels).to(config.device)\n",
        "model = ConvNeXtBaseEncoderUNet(config.num_classes, config.in_channels).to(config.device)\n",
        "# model = ConvNeXtEncoderUNet(config.num_classes, config.in_channels).to(config.device)\n",
        "# model = SwinEncoderUNet(config.num_classes, config.in_channels).to(config.device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        num_classes = preds.shape[1]\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "        mask = targets != 255\n",
        "        targets_cleaned = targets.clone()\n",
        "        targets_cleaned[~mask] = 0  # in-place操作を避けた\n",
        "\n",
        "        one_hot = F.one_hot(targets_cleaned, num_classes).permute(0, 3, 1, 2).float()\n",
        "        mask = mask.unsqueeze(1).float()\n",
        "        preds, one_hot = preds * mask, one_hot * mask\n",
        "        intersection = (preds * one_hot).sum(dim=(2, 3))\n",
        "        union = preds.sum(dim=(2, 3)) + one_hot.sum(dim=(2, 3))\n",
        "        return 1 - ((2 * intersection + self.smooth) / (union + self.smooth)).mean()\n",
        "\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    ce = F.cross_entropy(pred, target, ignore_index=255)\n",
        "    dice = DiceLoss()(pred, target)\n",
        "    return 0.5 * ce + 0.5 * dice\n",
        "\n",
        "model.train()\n",
        "for epoch in range(config.epochs):\n",
        "    total_loss = 0\n",
        "    for img, depth, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        img, depth, label = img.to(config.device), depth.to(config.device), label.to(config.device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type=config.device, enabled=torch.cuda.is_available()):\n",
        "            x = torch.cat((img, depth), dim=1)\n",
        "            pred = model(x)\n",
        "            if pred.shape[-2:] != label.shape[-2:]:\n",
        "                pred = F.interpolate(pred, size=label.shape[-2:], mode='bilinear', align_corners=True)\n",
        "            loss = combined_loss(pred, label)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ------------------\n",
        "#    Save + Predict\n",
        "# ------------------\n",
        "save_path = f\"model_{time.strftime('%Y%m%d_%H%M%S')}.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved:\", save_path)\n",
        "\n",
        "model.eval()\n",
        "def predict_tta(model, img, depth):\n",
        "    with torch.no_grad():\n",
        "        x = torch.cat((img, depth), dim=1)\n",
        "        pred1 = model(x)\n",
        "        pred2 = model(torch.flip(x, dims=[3]))\n",
        "        pred2 = torch.flip(pred2, dims=[3])\n",
        "        pred = (pred1 + pred2) / 2\n",
        "        pred = F.interpolate(pred, size=img.shape[-2:], mode='bilinear', align_corners=True)\n",
        "        return pred.argmax(dim=1)\n",
        "\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for img, depth in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
        "        img, depth = img.to(config.device), depth.to(config.device)\n",
        "        pred = predict_tta(model, img, depth)\n",
        "        preds.append(pred.cpu())\n",
        "\n",
        "np.save(\"submission.npy\", torch.cat(preds).numpy())\n",
        "print(\"submission.npy saved\")\n"
      ],
      "metadata": {
        "id": "H7km8vOv9lJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "#    Evaluation with TTA + ZIP submission\n",
        "# ------------------\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ 保存ファイル名とパスの設定\n",
        "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
        "model_path = f\"model_{timestamp}.pt\"\n",
        "submission_npy_path = \"submission.npy\"\n",
        "submission_zip_path = os.path.join(\"/content\", \"submission.zip\")\n",
        "notebook_path = \"/content/DL_Basic_2025_Competition_NYUv2_baseline.ipynb\"  # 任意に変更\n",
        "\n",
        "# ✅ モデル保存\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"✅ Saved model to: {model_path}\")\n",
        "\n",
        "# ✅ モデル読み込み\n",
        "device = config.device\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ✅ TTA予測関数（HFlip, VFlip, 90度回転）\n",
        "def predict_tta(model, img, depth):\n",
        "    with torch.no_grad():\n",
        "        x = torch.cat((img, depth), dim=1)\n",
        "\n",
        "        # 各方向で予測\n",
        "        pred1 = model(x)  # Original\n",
        "\n",
        "        x_h = torch.flip(x, dims=[3])  # HFlip\n",
        "        pred2 = torch.flip(model(x_h), dims=[3])\n",
        "\n",
        "        x_v = torch.flip(x, dims=[2])  # VFlip\n",
        "        pred3 = torch.flip(model(x_v), dims=[2])\n",
        "\n",
        "        x_r = x.rot90(1, dims=[2, 3])  # 90度回転\n",
        "        pred4 = model(x_r).rot90(-1, dims=[2, 3])\n",
        "\n",
        "        # 平均\n",
        "        pred = (pred1 + pred2 + pred3 + pred4) / 4.0\n",
        "        pred = F.interpolate(pred, size=img.shape[-2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "        return pred.argmax(dim=1)\n",
        "\n",
        "# ✅ 推論ループ（TTA）\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for img, depth in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
        "        img, depth = img.to(device), depth.to(device)\n",
        "        pred = predict_tta(model, img, depth)\n",
        "        all_preds.append(pred.cpu())\n",
        "\n",
        "# ✅ submission.npy に保存\n",
        "all_preds_tensor = torch.cat(all_preds, dim=0)\n",
        "np.save(submission_npy_path, all_preds_tensor.numpy())\n",
        "print(f\"✅ Predictions saved to: {submission_npy_path}\")\n",
        "\n",
        "# ✅ ZIPファイル作成\n",
        "with ZipFile(submission_zip_path, mode=\"w\", compression=ZIP_DEFLATED, compresslevel=9) as zf:\n",
        "    zf.write(submission_npy_path, arcname=\"submission.npy\")\n",
        "    zf.write(model_path, arcname=os.path.basename(model_path))\n",
        "    if os.path.exists(notebook_path):\n",
        "        zf.write(notebook_path, arcname=os.path.basename(notebook_path))\n",
        "\n",
        "print(f\"✅ submission.zip saved to: {submission_zip_path}\")\n"
      ],
      "metadata": {
        "id": "Yxp4N5Du9n9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "#    ZIP 提出ファイル作成（ローカル保存用）\n",
        "# ------------------\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# --- パス設定 ---\n",
        "output_dir = \"/content\"\n",
        "submission_zip_path = os.path.join(output_dir, \"submission.zip\")\n",
        "submission_npy_path = os.path.join(output_dir, \"submission.npy\")  # 明示的にパスを記述\n",
        "notebook_path = os.path.join(output_dir, \"DL_Basic_2025_Competition_NYUv2_baseline.ipynb\")  # 必要に応じて変更\n",
        "model_path = model_path if 'model_path' in locals() else \"model_latest.pt\"  # fallback処理\n",
        "\n",
        "# --- 出力ディレクトリ確認 ---\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- ZIP 作成 ---\n",
        "with ZipFile(submission_zip_path, mode=\"w\", compression=ZIP_DEFLATED, compresslevel=9) as zf:\n",
        "    if os.path.exists(submission_npy_path):\n",
        "        zf.write(submission_npy_path, arcname=\"submission.npy\")\n",
        "        print(f\"✅ submission.npy を追加\")\n",
        "    else:\n",
        "        print(\"❌ submission.npy が存在しません\")\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        zf.write(model_path, arcname=os.path.basename(model_path))\n",
        "        print(f\"✅ モデル重み {model_path} を追加\")\n",
        "    else:\n",
        "        print(\"❌ モデルファイルが見つかりません\")\n",
        "\n",
        "    if os.path.exists(notebook_path):\n",
        "        zf.write(notebook_path, arcname=os.path.basename(notebook_path))\n",
        "        print(f\"✅ ノートブック {notebook_path} を追加\")\n",
        "    else:\n",
        "        print(\"⚠️ ノートブックファイルが見つかりません（任意）\")\n",
        "\n",
        "print(f\"✅ 完成: ZIPファイルを保存しました → {submission_zip_path}\")\n"
      ],
      "metadata": {
        "id": "aBE4wwPx9qVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}